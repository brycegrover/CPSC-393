# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NMuesTbjUZZFGGyQCfgJV4St2fgqgkMJ
"""

import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import keras as kb
from keras import regularizers
from keras.utils import to_categorical
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

df = pd.read_csv("apple_quality.csv").dropna()
feats = ["Size", "Weight", "Sweetness", "Crunchiness", "Juiciness", "Ripeness",	"Acidity"]
df[feats] = df[feats].apply(pd.to_numeric, errors='coerce')

df['Quality'] = pd.Categorical(df['Quality']).codes  #convert 'good' and 'bad' to 0 1

X = df[feats]
y = to_categorical(df['Quality'])

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=123)

z = StandardScaler()
X_train[feats] = z.fit_transform(X_train[feats])
X_test[feats] = z.transform(X_test[feats])

#early stopping
callbacks =[kb.callbacks.EarlyStopping(monitor = "val_accuracy", patience = 3)]

# model architecture
model = kb.Sequential([
    kb.layers.Dense(50, input_shape=(7,)),
    kb.layers.BatchNormalization(),
    kb.layers.Dense(75, activation="relu"),
    kb.layers.BatchNormalization(),
    kb.layers.Dropout(0.3),
    kb.layers.Dense(50, kernel_regularizer='l1'),
    kb.layers.BatchNormalization(),
    kb.layers.Dense(10, activation="relu"),
    kb.layers.Dense(2, activation="softmax")
])

# compile  model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# summary
model.summary()

# train model
model.fit(X_train, y_train, epochs=100,
          validation_data = (X_test, y_test),
          verbose=0,
          callbacks = callbacks)

# Evaluate the model
model.evaluate(X_test, y_test)

# get model predictions
y_pred_probs = model.predict(X_test)

# convert probs to  labels
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

# Calculate precision and recall (got code from sklearn documentation -- noted in readme.txt) https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
precision = precision_score(y_true, y_pred, average='binary')
recall = recall_score(y_true, y_pred, average='binary')

print("Precision: ", precision)
print("Recall: ", recall)

# prepare data for a decision tree model (keeping quality as a categorical variable and using the same data split as the NN)
X = df[feats]
y = df['Quality']  # quality stays categorical

#same split as the NN (123 random state)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# parameters to test
params = {
    'max_depth': [3, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 5, 10, 20]
}

dt = DecisionTreeClassifier()

# find best params
dt_grid = GridSearchCV(estimator=dt, param_grid = params, cv=10, scoring='accuracy')

# fit using best params
dt_grid.fit(X_train, y_train)

# print best params
print("Best parameters:", dt_grid.best_params_)

# best model from gid search
best_dt = dt_grid.best_estimator_

# make preds
y_pred = best_dt.predict(X_test)

# calc metrics
dt_accuracy = accuracy_score(y_test, y_pred)
dt_precision = precision_score(y_test, y_pred, average='binary')
dt_recall = recall_score(y_test, y_pred, average='binary')
# display metrics
print("Accuracy:", dt_accuracy)
print("Precision:", dt_precision)
print("Recall:", dt_recall)